# 3 System Analysis and Design

This chapter details the comprehensive design and architectural decisions behind the proposed Bangla Text-to-Speech (TTS) system. The system is engineered as a two-stage neural pipeline supported by a robust linguistic front-end, specifically tailored to address the complexities of the Bangla language while ensuring high-fidelity, natural-sounding synthesis.

## 3.1 Requirement Analysis

The primary objective of the system is to generate human-like speech from raw Bangla text. To achieve this, the system must satisfy specific functional and non-functional requirements derived from the need for high intelligibility and computational efficiency.

**Functional Requirements:**
*   **Text Normalization:** The system must accept raw Bangla text (UTF-8) and correctly normalize non-standard words (NSWs) such as numbers, dates (DD-MM-YYYY), time formats, and currency into their spoken graphemic forms.
*   **Phonetic Conversion:** The system must convert normalized graphemes into the International Phonetic Alphabet (IPA) to handle Bangla’s lack of one-to-one mapping between spelling and pronunciation (e.g., the inherent vowel /ɔ/).
*   **Mel-Spectrogram Generation:** The acoustic model must generate high-resolution mel-spectrograms (80 mel bands) from phonetic inputs, capturing prosody, intonation, and duration.
*   **Waveform Synthesis:** The system must generate a 22,050 Hz, 16-bit PCM mono WAV file from the mel-spectrograms without introducing artifacts or robotic buzzing.

**Non-Functional Requirements:**
*   **Naturalness:** The synthesized speech must achieve a Mean Opinion Score (MOS) comparable to state-of-the-art systems (target MOS > 4.0).
*   **Inference Speed:** The vocoder must function faster than real-time (Real-Time Factor < 1.0) on a standard GPU (e.g., NVIDIA T4 or RTX series) to allow for potential practical deployment.
*   **Voice Stability:** The system must be robust against long sentences, avoiding the "mumbling" or attention-collapse issues common in autoregressive models.

## 3.2 System Overview / Architecture

The proposed solution implements a modular pipeline approach. This decoupling allows for independent optimization of the text processing, acoustic modeling, and waveform generation components.

### 3.2.1 High-Level Architecture

The end-to-end pipeline consists of three distinct modules:
1.  **Linguistic Front-End:** A rule-based text normalizer followed by a Grapheme-to-Phoneme (G2P) converter.
2.  **Acoustic Model (Flowtron):** An autoregressive flow-based generative network that maps linguistic features to a latent space and subsequently to mel-spectrograms.
3.  **Neural Vocoder (HiFi-GAN):** A Generative Adversarial Network that converts mel-spectrograms into time-domain audio waveforms.

**[Figure 3.1 Placeholder]**
*Guidance: Insert a block diagram showing the data flow from left to right: Raw Text -> [Text Normalizer] -> Normalized Text -> [G2P] -> Phonemes -> [Flowtron] -> Mel-Spectrogram -> [HiFi-GAN] -> Audio Waveform.*

### 3.2.2 Detailed Design Diagrams

**The Acoustic Model: Flowtron**
Flowtron is chosen for its ability to model the complex distribution of speech data by maximizing the likelihood of the training data. Unlike traditional attention-based models, Flowtron utilizes normalizing flows.

*   **Encoder:** The text encoder modifies the Tacotron architecture by replacing batch normalization with instance normalization.
*   **Flow Steps:** The core generation utilizes a series of invertible affine coupling layers. In each step, the model processes the mel-spectrogram sequence. The affine coupling layer produces scale ($s$) and bias ($b$) terms using a neural network $NN()$ that conditions on the input text and previous timesteps.
*   **Latent Space ($z$):** The model learns an invertible mapping to a latent space $z$, parametrized by a spherical Gaussian $P(z)$. This allows for control over speech variation by manipulating the latent space during inference.

**[Figure 3.2 Placeholder]**
*Guidance: Insert a diagram of the Flowtron architecture (similar to Figure 1 in the Flowtron paper). It should show the "Affine Coupling Layer," the flow of "x" (Mel-spectrogram) and "z" (Latent), and the concatenation of text embeddings.*

**The Vocoder: HiFi-GAN**
HiFi-GAN is employed to invert the mel-spectrograms generated by Flowtron into audio. It is designed for high computational efficiency without sacrificing audio quality.

*   **Generator:** The generator is a fully convolutional neural network. It uses a **Multi-Receptive Field Fusion (MRF)** module. This module observes patterns of various lengths in parallel by summing outputs from multiple residual blocks with different kernel sizes and dilation rates.
*   **Discriminators:** Two types of discriminators are used to train the generator adversarially:
    1.  **Multi-Period Discriminator (MPD):** Reshapes the 1D audio into 2D structures to capture periodic patterns in speech (crucial for pitch).
    2.  **Multi-Scale Discriminator (MSD):** Operates on the audio at different temporal scales (raw, x2 average-pooled, x4 average-pooled) to ensure audio quality across frequency bands.

**[Figure 3.3 Placeholder]**
*Guidance: Insert the HiFi-GAN architecture diagram (similar to Figure 1 in the HiFi-GAN paper). Highlight the "Mel-Spectrogram Input," the "Upsampling" blocks, and the "MRF" (Multi-Receptive Field Fusion) module.*

### 3.2.3 Design Alternatives and Rationale

**Acoustic Model: Flowtron vs. Tacotron 2**
While Tacotron 2 is a standard benchmark, it often suffers from attention instability (skipping or repeating words) and lacks control over prosody. Flowtron was selected because:
1.  **Invertibility:** It provides an exact likelihood calculation, making training stable.
2.  **Control:** It allows for manipulation of the latent space $z$ to control pitch, tone, and speech rate without retraining, which is vital for expressive Bangla speech.

**Vocoder: HiFi-GAN vs. WaveNet/WaveGlow**
*   **WaveNet:** Although high quality, it is autoregressive and prohibitively slow for inference (generating one sample at a time).
*   **WaveGlow:** Is flow-based and fast but requires immense GPU memory (VRAM) due to its depth.
*   **HiFi-GAN:** Was selected as the optimal choice because it achieves state-of-the-art fidelity (MOS scores comparable to human speech) while being orders of magnitude faster and more memory-efficient than WaveGlow, thanks to its non-autoregressive GAN architecture.

## 3.3 Algorithmic Flow / Pseudocode/Mathematical Model

The system relies on rigorous mathematical foundations from Normalizing Flows and Adversarial learning.

**Flowtron Mathematical Model**
Flowtron maximizes the log-likelihood of the data. It transforms a simple distribution $p(z)$ (Gaussian) into a complex data distribution $p(x)$ (Mel-spectrogram) using a series of invertible transformations $f$.

The change of variables formula used for the loss function is:
$$ \log p_\theta(x) = \log p_\theta(z) + \sum_{i=1}^{k} \log |\det(J(f^{-1}_i(x)))| $$

Where $z$ is obtained by passing $x$ through the inverse flow steps:
$$ z = f^{-1}_k \circ f^{-1}_{k-1} \circ \dots \circ f^{-1}_0(x) $$

The **Affine Coupling Layer** ensures invertibility. For an input $x$, it computes:
$$ (\log s_t, b_t) = NN(x_{1:t-1}, \text{text}) $$
$$ x'_t = s_t \odot x_t + b_t $$

**HiFi-GAN Mathematical Model**
HiFi-GAN is trained using a composite loss function involving adversarial objectives and feature matching.

The **GAN Loss** (Least Squares GAN) forces the generator ($G$) to fool the discriminator ($D$):
$$ \mathcal{L}_{Adv}(D; G) = \mathbb{E}_{(x, s)} [(D(x) - 1)^2 + (D(G(s)))^2] $$

To prevent the common artifacts of GANs, a **Feature Matching Loss** is applied, which minimizes the L1 distance between the discriminator's internal feature maps for real and synthetic audio:
$$ \mathcal{L}_{FM}(G; D) = \mathbb{E}_{(x, s)} \left[ \sum_{i=1}^{T} \frac{1}{N_i} || D^i(x) - D^i(G(s)) ||_1 \right] $$

Additionally, a **Mel-Spectrogram Loss** ensures the spectral accuracy of the generated waveform:
$$ \mathcal{L}_{Mel}(G) = \mathbb{E}_{(x, s)} [ || \phi(x) - \phi(G(s)) ||_1 ] $$

The final objective combines these weighted losses to ensure both structural consistency and spectral fidelity.

**[Figure 3.4 Placeholder]**
*Guidance: Insert a flowchart representing the training loop. It should show the Flowtron Loss calculation (maximizing likelihood) running in parallel with the HiFi-GAN Adversarial training loop (Generator vs. Discriminator).*

## 3.4 Summary

This chapter established the system architecture for a high-fidelity Bangla TTS. By analyzing the requirements, we designed a pipeline that integrates a specific rule-based front-end with a Flowtron acoustic model for expressive control and a HiFi-GAN vocoder for efficient, high-quality waveform synthesis. The mathematical models defined here provide the basis for the implementation and training procedures detailed in the subsequent chapters.
